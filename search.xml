<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>TomoProject</title>
      <link href="/2021/09/16/TomoProject/"/>
      <url>/2021/09/16/TomoProject/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>It is a project to use Convolutional Neural Network(CNN) to recognize how many objects in a 3D image and findout the superficial areas for all the objects. First step, only spheres will be applied in images.</p><p>Therefore, the first step has two tasks. 1, creating the dataset with TomoPhantom; 2, design and training the CNN network. One of the challenges is the images can have multiple different spheres and the output need to recognize how many spheres are there in the image with each of their sizes. Image classification is possble to be employed in this case, but with number of spheres increasing, it is not ideal plan to have all possible classes for each image groups. Object Recognition or Object Detection might be better operations, but the output has to be a conclution instead of operated images. We might need a new network hybridize these.</p><p>TomoPhantom can be used to generate the images. The superficial areas will be labeled for the training dataset. Then train a Convolutional Neural Network to findout the superficial area for spheres.</p><p><a href="https://github.com/dkazanc/TomoPhantom" target="_blank" rel="noopener">The Link for TomoPhantom</a></p><h2 id="Generate-Spheres-in-3D-Images"><a href="#Generate-Spheres-in-3D-Images" class="headerlink" title="Generate Spheres in 3D Images"></a>Generate Spheres in 3D Images</h2><p>There is a paper introduced about TomoPhantom. <a href="https://www.sciencedirect.com/science/article/pii/S2352711018300335" target="_blank" rel="noopener">TomoPhantom, a software package to generate 2D–4D analytical phantoms for CT image reconstruction algorithm benchmarks</a>. This paper introduced the advantages of TomoPhantom and some examples to employ it. With TomoPhantom, models and objects can be generated. Model is a group objects created together. If yoou have a special of group objects need to be quoted multiple times, you should generate with a model. I recommend to check <a href="https://github.com/dkazanc/TomoPhantom/blob/master/PhantomLibrary/models/Phantom3DLibrary.dat" target="_blank" rel="noopener">Phantom3DLibrary.dat</a> to understand generate models. This page has the settings for all of the models avaliable with Tomo’s library. For this porject, we only need to <a href="https://github.com/dkazanc/TomoPhantom/blob/master/Demos/Python/3D/Object3D.py" target="_blank" rel="noopener">gererate objects</a>. We can use the model to create random spheres, such as hundreds of spheres in one model, but it is not necessary. For function Objects3D, there are 6 modes can be chosen: “CONE”, “CUBOID”,”ELLIPCYINEDER”,”ELLIPSOID”, “GAUSSIAN”, “PARABOLOID”. We only need “ELLIPSOID” for this project. ‘x0’,’y0’,’z0’, in the range of [-1.0,1.0], set up the location. ‘a’,’b’,’c’, in the range of [0,1], set up the size of the object. ‘phi1’ is the angle.</p><h2 id="Machine-Vision-Technology"><a href="#Machine-Vision-Technology" class="headerlink" title="Machine Vision Technology"></a>Machine Vision Technology</h2><p>There are some popular Machine Vision technology, such as Image Classification, Image Reconstruction, Deblurring, Denoise, Image Segmentation, Visual Odometry, Object Detection , and Object Recognition. Object Detection, Visual Odometry, and Object Recognition are the topics I haven’t used before. However, this project is special from any work I have learned. Personally, Image Classification and Object Recognition can be emploied in this project. Both of them have their advantages and weakpoints. </p><p>First of all, let’s check Object Recognition and notice the difference between it and Object Detection. The resouse is from <a href="https://dsp.stackexchange.com/questions/12940/object-detection-versus-object-recognition" target="_blank" rel="noopener">this link</a>.Object Recognition is used to answer this question: which object is depicted in the image? Object Detection, on other hand, is answering: where is this object in the image? Input for Object Recognition is images with unknown objects. The output for it is the positions and labels.</p><p>Under Object Detection topic, <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf" target="_blank" rel="noopener">You Only Look Once(YOLO)</a> is one of the most famorous </p><h2 id="Data-Storing"><a href="#Data-Storing" class="headerlink" title="Data Storing"></a>Data Storing</h2><p>After creating the images, they need to be stored and to be built into a reusable database. For most medical 3D images are stolded into Nifti format, Minc format, or Dicom format. Nifti format is common to be used for CNN.<br><img src="/images/MedicalImageFormats.PNG" alt=""><br>There are some popular 3D images database such as modelNet10, which is using .OFF file format. Some database are real photos with objects in different angles such as ObjectNet3D, MVTecITODD, and T-LESS. HDF5 is another format for storing </p><p>The images are simulating a 10cm<em>10cm</em>10cm space. The image itself is 128^3 voxels 3D image. 10mm radius uses 12 picels. 5mm uses 6 pixels. 2.5mm use 3 pixels. That means except the spheres most part of image will be empty. In this case, OctNet is an arithmetic can help optimize the storage space and training time. It created an unbalanced octrees arithmetic to hierarchically partition sparse 3D image. With OctNet, this project might can increase resolutions to 256^3 voxels.</p><h2 id="Building-Datasets"><a href="#Building-Datasets" class="headerlink" title="Building Datasets"></a>Building Datasets</h2><h2 id="Images-Classification"><a href="#Images-Classification" class="headerlink" title="Images Classification"></a>Images Classification</h2><h2 id="Object-Recognition"><a href="#Object-Recognition" class="headerlink" title="Object Recognition"></a>Object Recognition</h2><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3948928/" target="_blank" rel="noopener">Medical Image File Formats</a>,Michele Larobina and Loredana Murino ,<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3948928/" target="_blank" rel="noopener">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3948928/</a></li><li><a href="https://cvgl.stanford.edu/papers/xiang_eccv16.pdf" target="_blank" rel="noopener">ObjectNet3D:A Large Scale Database for 3D Object Recognition</a>,Yu Xiang, Wonhui Kim, etc.,<a href="https://cvgl.stanford.edu/projects/objectnet3d/" target="_blank" rel="noopener">https://cvgl.stanford.edu/projects/objectnet3d/</a></li><li><a href="https://towardsdatascience.com/image-data-labelling-and-annotation-everything-you-need-to-know-86ede6c684b1" target="_blank" rel="noopener">Image Data Labelling and Annotation – Everything you need to know</a>, Sabina Pokhrel, <a href="https://towardsdatascience.com/image-data-labelling-and-annotation-everything-you-need-to-know-86ede6c684b1" target="_blank" rel="noopener">https://towardsdatascience.com/image-data-labelling-and-annotation-everything-you-need-to-know-86ede6c684b1</a></li><li><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf" target="_blank" rel="noopener">You Only Look Once: Unified, Real-Time Object Detection</a>, Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi, <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf" target="_blank" rel="noopener">https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf</a></li><li><a href="https://arxiv.org/ftp/arxiv/papers/2006/2006.14837.pdf" target="_blank" rel="noopener">Expandable YOLO: 3D Object Detection from RGB-D Image</a>, Masahiro Takahashi, Alessandro Moro, Yonghoon, Kazunori Umeda, <a href="https://arxiv.org/ftp/arxiv/papers/2006/2006.14837.pdf" target="_blank" rel="noopener">https://arxiv.org/ftp/arxiv/papers/2006/2006.14837.pdf</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> ImageProcess </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Convolutional_Neural_Network, Python, ME Application </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Paper Reading Notes in Machine Vision Area</title>
      <link href="/2021/09/10/Paper-Notes-in-Machine-Vision-Area/"/>
      <url>/2021/09/10/Paper-Notes-in-Machine-Vision-Area/</url>
      
        <content type="html"><![CDATA[<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>This blog is going to be used to follow up the paper I read about sensor topics. Therefore, I can go back and check easily if I need to review or remember some topics.</p><hr><h3 id="Dataset-Information"><a href="#Dataset-Information" class="headerlink" title="Dataset Information"></a>Dataset Information</h3><p>For the projects, I am searching for more informtion about dataset used for Machine Vision topic. In master degree, I was using the dataset directly for researching. However, at this point I have to build a dataset for the special task. Therefore, I have to research how to create the dataset can be used. The project is similar to diagnoisis cancer from medical images, so I began from there.</p><p><a href="https://www.sciencedirect.com/science/article/pii/S2352914819301133" target="_blank" rel="noopener">“Cancer Diagnosis in histopathological image: CNN based approach”</a><br>Datasets for breast cancer: Breast Cancer for breast (WDBC) cancer Wisconsin Original Data Set (UC Irvine Machine Learning Repository), MITOS- ATYPIA-14, and BreakHis.<br>This used the labeled (benign/malignant) input image form the raw pixels and highlighted the visual patterns, and then utilize those patterns to distinguish between the visual patterns, and then utilize those patterns to distinguish betwwen non-cancerous and cancer containing tissue, working akin to digital staining, which spotlights image segments crucial for diagnostic decisions, with the help of a classifier network.<br>Most of the pixels in the image are redundant and do not contribute substantially to the intrinsic information of an image. While dealing with AI networks, it is required to eliminate them to avoid unnecessary computational overhead. This can be achieved by compression techniques. We begin the implementation of our deep net by processing the images in the dataset.<br>Feature learning is a crucial step in the classification process for both human and machine algorithm. A study has shown that the human brain is sensitive to shapes, while computers are more sensitive to patterns and texture.</p><p><a href="https://ieeexplore.ieee.org/document/7312934" target="_blank" rel="noopener">“A Dataset for Breast Cancer Histopathological Image Classification”</a></p><p>For the project, dataset will use the synthetic data. A study has shown that the human brain is sensitive to shapes, while computers are more sensitive to patterns and texture. I am goint to check how it might will affect the training for our project.<br><a href="https://openreview.net/forum?id=Bygh9j09KX" target="_blank" rel="noopener">“ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness”</a></p>]]></content>
      
      
      <categories>
          
          <category> Reading Notes </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Paper Reading Notes in Sensor Area</title>
      <link href="/2021/09/10/Paper-Notes-in-Sensor-Area/"/>
      <url>/2021/09/10/Paper-Notes-in-Sensor-Area/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>This blog is going to be used to follow up the paper I read about sensor topics. Therefore, I can go back and check easily if I need to review or remember some topics.</p><hr><h2 id="2021-Fall"><a href="#2021-Fall" class="headerlink" title="2021 Fall"></a>2021 Fall</h2><h3 id="Determination-of-Dynamic-History-of-Impact-Loadings-Using-Polyvinylidene-Fluoride-PVDF-Films"><a href="#Determination-of-Dynamic-History-of-Impact-Loadings-Using-Polyvinylidene-Fluoride-PVDF-Films" class="headerlink" title="Determination of Dynamic History of Impact Loadings Using Polyvinylidene Fluoride (PVDF) Films"></a>Determination of Dynamic History of Impact Loadings Using Polyvinylidene Fluoride (PVDF) Films</h3><h3 id="A-New-Calibration-Method-for-Dynamically-Loaded-Transducers-and-Its-Application-to-Cavitation-Impact-Measurement"><a href="#A-New-Calibration-Method-for-Dynamically-Loaded-Transducers-and-Its-Application-to-Cavitation-Impact-Measurement" class="headerlink" title="A New Calibration Method for Dynamically Loaded Transducers and Its Application to Cavitation Impact Measurement"></a>A New Calibration Method for Dynamically Loaded Transducers and Its Application to Cavitation Impact Measurement</h3><h3 id="Impact-Damage-Detection-in-Composite-Laminates-Using-PVDF-and-PZT-Sensor-Signals"><a href="#Impact-Damage-Detection-in-Composite-Laminates-Using-PVDF-and-PZT-Sensor-Signals" class="headerlink" title="Impact Damage Detection in Composite Laminates Using PVDF and PZT Sensor Signals"></a>Impact Damage Detection in Composite Laminates Using PVDF and PZT Sensor Signals</h3><h3 id="PVDF-Pressure-Transducers-for-Shock-Wave-and-Explosives-Research"><a href="#PVDF-Pressure-Transducers-for-Shock-Wave-and-Explosives-Research" class="headerlink" title="PVDF Pressure Transducers for Shock Wave and Explosives Research"></a>PVDF Pressure Transducers for Shock Wave and Explosives Research</h3>]]></content>
      
      
      <categories>
          
          <category> Reading Notes </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Simulation Result From Tecplot Analyse</title>
      <link href="/2021/09/08/Simulation-Result-From-Tecplot-Analyse/"/>
      <url>/2021/09/08/Simulation-Result-From-Tecplot-Analyse/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>This project is analyse a 3D space, such as an engine. There is a flammable liquate shot into this space burned with air. The goal is using Convolutional Neural Network(CNN) to analysis wheter there are some voxels unusual. The data I got is a 21<em>132,300 array. They are describing a 0.1m</em>0.1m*0.1m cube space. The first three columns are the 3 directions distance from origin point. They locates totally 132,300 points in this cube space. The following cubes are the valures for every point. For example, the temperature, pressure, liquite speed, etc. Therefore, there are two steps for this project. 1st: build up the dataset. 2nd: tranning and test with CNN.</p><hr><h2 id="Matrix-interpolating"><a href="#Matrix-interpolating" class="headerlink" title="Matrix interpolating"></a>Matrix interpolating</h2><p>The first idea I have is converting the input data into 3D matrixes with different values. For example, a matixes each pixel is describing temperter in this space. These matrixes will be similar with the 3D images I worked before. Getting close to the coordinates, they are not perfect numbers. For the X coordinate, the biggest number is 1.000000015E-01 and the first non-zero number is 2.722533187E-03.If we treat 2.722533187E-03 as the smallest unit, there are 36.73 units compare with 1.000000015E-01. Which is not integer. Also the totally point 36^3 is much smaller than 132,300. I checked how many different numbers in each coordinates. Turns out X has 30,305, Y has 97, Z has 30,466 different elements in their list. To reach this goal, transfer into a matrix, there are some functions I need to study for.</p><h3 id="scipy-interpolate-griddata"><a href="#scipy-interpolate-griddata" class="headerlink" title="scipy.interpolate.griddata"></a>scipy.interpolate.griddata</h3><p>numpy has some functions can convert 1D arraies into 1 nD array. However, the task actually is writing data into a 3D array. Therefore, scipy.interpolate is necessary.<br><a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.griddata.html" target="_blank" rel="noopener">Link to griddata</a><br>This function is used to interpolate unstructured D-D data. It included 3 method. The source cod is not very long. To understand it, I checked the functions emploied in it. They are scipy.spatial.cKDTree (it is same with KDTree now), numpy.argsort, numpy.asarray, numpy.meshgrid, numpy.linspace.</p><p><a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.NearestNDInterpolator.html#scipy.interpolate.NearestNDInterpolator" target="_blank" rel="noopener">Link to NearestNDInterpolator</a><br><a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.LinearNDInterpolator.html#scipy.interpolate.LinearNDInterpolator" target="_blank" rel="noopener">Link to LinearNDInterpolator</a><br><a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.CloughTocher2DInterpolator.html#scipy.interpolate.CloughTocher2DInterpolator" target="_blank" rel="noopener">Link to CloughToucher2DInterpolator</a></p><hr><p>###</p>]]></content>
      
      
      <categories>
          
          <category> ImageProcess </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Convolutional_Neural_Network, Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Polyvinylidene Fluoride(PVDF) Films Based Pressure Sensor</title>
      <link href="/2021/09/08/Polyvinylidene-Fluoride-PVDF-Films-Based-Pressure-Sensor/"/>
      <url>/2021/09/08/Polyvinylidene-Fluoride-PVDF-Films-Based-Pressure-Sensor/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>This project is designing a researching instrument for testing a new material. The challenge is the researching plan is not clear, so we need to adjust it while the work is keeping going. </p><hr><h2 id="Frist-phase"><a href="#Frist-phase" class="headerlink" title="Frist phase"></a>Frist phase</h2><h3 id="Goal"><a href="#Goal" class="headerlink" title="Goal:"></a>Goal:</h3><p>Design a circuit use PVDF sensor and show the signal from oscilloscope. Let a steel ball fall on the sensor.</p><h3 id="Challenge"><a href="#Challenge" class="headerlink" title="Challenge:"></a>Challenge:</h3><ol><li>The output wave in oscilloscope has too much noise.</li><li>There is a clear maximum value on oscilloscope.</li><li>The sensor cannot really tell how strong the force add on it. No matter how strong the force add on the sensor, the output wave is similar with each other.<h3 id="Advices-to-adjust"><a href="#Advices-to-adjust" class="headerlink" title="Advices to adjust:"></a>Advices to adjust:</h3></li><li>Replace battery with power source.</li><li>Calibrating the Oscilloscope Probe.</li><li>Adjust the sensor and steel ball design. Make sure the sensor won’t be broken </li><li>Check circuit and replace the broken elements. </li></ol>]]></content>
      
      
      <categories>
          
          <category> Hardware </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Instrument Design, Hardware, Research </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2021/09/08/hello-world/"/>
      <url>/2021/09/08/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>The Shortest Paths Algorithms</title>
      <link href="/2020/11/17/The-Shortest-Paths-Algorith/"/>
      <url>/2020/11/17/The-Shortest-Paths-Algorith/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>The shortest paths algorithms are the problems I touched through school work. There are many algorithms I learned through classes. I believe they will be useful for the future. Since I spent time to understand them, I think it is worth to spent sometime to collect and organize them here. It will be easy for me to review them later.</p><hr><h2 id="Bread-First-Search"><a href="#Bread-First-Search" class="headerlink" title="Bread First Search"></a>Bread First Search</h2><hr><h2 id="Depth-First-Search"><a href="#Depth-First-Search" class="headerlink" title="Depth First Search"></a>Depth First Search</h2><hr><h2 id="Dijkstra’s-Algorithm"><a href="#Dijkstra’s-Algorithm" class="headerlink" title="Dijkstra’s Algorithm"></a>Dijkstra’s Algorithm</h2><hr><h2 id="Prim’s-Algorithm"><a href="#Prim’s-Algorithm" class="headerlink" title="Prim’s Algorithm"></a>Prim’s Algorithm</h2><hr><h2 id="Kruskal’s-Algorithm"><a href="#Kruskal’s-Algorithm" class="headerlink" title="Kruskal’s Algorithm"></a>Kruskal’s Algorithm</h2><hr><h2 id="Bellman-Ford’s-Algorithm"><a href="#Bellman-Ford’s-Algorithm" class="headerlink" title="Bellman-Ford’s Algorithm"></a>Bellman-Ford’s Algorithm</h2><hr><h2 id="Floyd-Warshall-Algorithm"><a href="#Floyd-Warshall-Algorithm" class="headerlink" title="Floyd-Warshall Algorithm"></a>Floyd-Warshall Algorithm</h2>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> School, Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The Longest Paths Algorithm</title>
      <link href="/2020/11/17/The-Longest-Paths-Algorithm/"/>
      <url>/2020/11/17/The-Longest-Paths-Algorithm/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>The Liao-Wong Algorithm and Bellman-Ford Algorithm are two algorithms for longest paths problem I touched through school work. There are many algorithms I learned through classes. I believe they will be useful for the future. Since I spent time to understand them, I think it is worth to spent sometime to collect and organize them here. It will be easy for me to review them later.</p><p>There are a lot of algorithm for shortest paths. Longest paths are a little bit complex problem to handle.</p><hr><h2 id="Bellman-Ford-Algorithm"><a href="#Bellman-Ford-Algorithm" class="headerlink" title="Bellman-Ford Algorithm"></a>Bellman-Ford Algorithm</h2><p>Bellman-Ford Algorithm is very straight forward algorithm from shortest paths. The idea is switching the positive and negative weights for the paths. Then using the shortest paths algorithm to find out the shortest path. Change it pack to positive weight. It is the longest path.</p><hr><h2 id="Liao-Wong-Algorithm"><a href="#Liao-Wong-Algorithm" class="headerlink" title="Liao-Wong Algorithm"></a>Liao-Wong Algorithm</h2><p>Liao-Wong Algorithm needs to ignore the negative weight paths firstly. Find the longest path only with the positive weight paths. Then adding the negative weight paths and adjust the solutions. </p>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> School, Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The Kernighan-Lin Partitioning Algorithm</title>
      <link href="/2020/11/17/The-Kernighan-Lin_Partitioning-Algorithm/"/>
      <url>/2020/11/17/The-Kernighan-Lin_Partitioning-Algorithm/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>The Kernighan-Lin Algorithm is an algorithm I touched through school work. There are many algorithms I learned through classes. I believe they will be useful for the future. Since I spent time to understand them, I think it is worth to spent sometime to collect and organize them here. It will be easy for me to review them later.</p><hr><h2 id="Problem-For-This-Algorithm"><a href="#Problem-For-This-Algorithm" class="headerlink" title="Problem For This Algorithm"></a>Problem For This Algorithm</h2><p>There is an edge-weighted undirected graph G(V,E); the graph has 2n vertices(V=2n); an edge(a,b) has weight Rab. The goal is to minimize the total weight of the edges cut by the partitioning of V into the sets A and B.</p><hr><h2 id="The-Principle"><a href="#The-Principle" class="headerlink" title="The Principle"></a>The Principle</h2><p>The algorithm is starting with an initial partitial consisting of the sets A0 and B0. In an iterative process, subsets of both sets are isolated and interchanged. The iteration goes on until no improvement in the cut cost is possible.</p>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> School, Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CAD Techniques VLSI Design Homework</title>
      <link href="/2020/10/02/CAD-Techniques-VLSI-DSGN-HW/"/>
      <url>/2020/10/02/CAD-Techniques-VLSI-DSGN-HW/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>ECE588 is CAD Techniques VLSI Design. It is one class I am taking this semester. I am writing this blog to review my homeworks and projects I have finished in this class. Before the exam I am going to create another blog for reviewing everything I learned in this lecture.</p><hr><h2 id="Homework-1"><a href="#Homework-1" class="headerlink" title="Homework 1"></a>Homework 1</h2><p>The first homework is about graph theory algorithms. Algorithmic graph is used to understand and formulate algorithm. There are 7 algorithms I worked with C language to run and see how these graph theory algorithms work.</p><h3 id="1-Breath-First-Search"><a href="#1-Breath-First-Search" class="headerlink" title="1. Breath First Search"></a>1. Breath First Search</h3><p>Breath First Search is used to search the target vertex from the start vertex. This algorithm check vertices through edges from the vertices around the start vertex. Then it move to the two steps away vertices. It needs to pay attention every vertex should only be count for one time.</p><h3 id="2-Depth-First-Search"><a href="#2-Depth-First-Search" class="headerlink" title="2. Depth First Search"></a>2. Depth First Search</h3><p>Depth First Search is similar with Breath First Search. Only difference is from the start vertex it move through edges to the end, then move into the next beside vertex. It works like maze. Trying another path until goes into the end.</p><h3 id="3-Dijkstra’s-Algorithm"><a href="#3-Dijkstra’s-Algorithm" class="headerlink" title="3. Dijkstra’s Algorithm"></a>3. Dijkstra’s Algorithm</h3><p>Dijkstra’s Algorithm is used to find out the shortest path to the task vertex. It works for the graphs have weight on their edges. From the start vertex, it find out the smallest weight to the start vertex in one step. Next step find the second smallest weight to the start vertex. Until all vertices are all went through.</p><h3 id="4-Prim’s-Algorithm"><a href="#4-Prim’s-Algorithm" class="headerlink" title="4. Prim’s Algorithm"></a>4. Prim’s Algorithm</h3><p>Prim’s Algorithm also is used to find out the shortest path. It is growing a tree to cover all the vertices. Need to pay attention that Prim’s Algorithm should not have loops on the growing tree. Every step the tree will grow to another vertex which has the smallest weight on the tree.</p><h3 id="5-Kruskal’s-Algorithm"><a href="#5-Kruskal’s-Algorithm" class="headerlink" title="5. Kruskal’s Algorithm"></a>5. Kruskal’s Algorithm</h3><p>Kruskal’s Algorithm is growing a forest instead of tree. The difference is it focus on edge instead of vertices. Prim’s Algorithm is a connecting tree. Kruskal’s Algorithm does not necessary to be connect. It always chose the smallest weight edge until the forest connect all the vertices.</p><h3 id="6-Bellman-Ford’s-Algorithm"><a href="#6-Bellman-Ford’s-Algorithm" class="headerlink" title="6. Bellman Ford’s Algorithm"></a>6. Bellman Ford’s Algorithm</h3><p>Bellman Ford’s Algorithm is one way to find out the shortest paths to the start vertex from all vertices. If there are n vertices in the graph, it has n-1 steps. At the beginning, every vertex has infinite weight to the start vertex, except itself. Each step update the smallest weight to the start vertex. At the end of this algorithm, all the weight to the start vertex is the shortest length.</p><h3 id="7-Floyd-Warshall-Algorithm"><a href="#7-Floyd-Warshall-Algorithm" class="headerlink" title="7. Floyd Warshall Algorithm"></a>7. Floyd Warshall Algorithm</h3><h2 id="Floyd-Warshall-Algorithm-is-used-to-find-out-the-shortest-paths-to-any-two-vertices-One-problem-for-it-is-Floyd-Warshall-Algorithm-cost-a-long-time-to-run"><a href="#Floyd-Warshall-Algorithm-is-used-to-find-out-the-shortest-paths-to-any-two-vertices-One-problem-for-it-is-Floyd-Warshall-Algorithm-cost-a-long-time-to-run" class="headerlink" title="Floyd Warshall Algorithm is used to find out the shortest paths to any two vertices. One problem for it is Floyd Warshall Algorithm cost a long time to run."></a>Floyd Warshall Algorithm is used to find out the shortest paths to any two vertices. One problem for it is Floyd Warshall Algorithm cost a long time to run.</h2><h2 id="Homework-2"><a href="#Homework-2" class="headerlink" title="Homework 2"></a>Homework 2</h2>]]></content>
      
      
      <categories>
          
          <category> School Class </category>
          
      </categories>
      
      
        <tags>
            
            <tag> School, CAD, VLSI, C </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Computer Vision and Image Processing</title>
      <link href="/2020/08/18/Computer-Vision-and-Image_processing/"/>
      <url>/2020/08/18/Computer-Vision-and-Image_processing/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>For my first year of master degree studying, I was focusing on Image Processing and Computer Vision topics. In this blog I am going to edit everything I learned about these two topics. By doing this I can have a chance to review the details I have learned.</p><hr><h2 id="What-is-the-differences-between-Image-Processing-and-Computer-Vision"><a href="#What-is-the-differences-between-Image-Processing-and-Computer-Vision" class="headerlink" title="What is the differences between Image Processing and Computer Vision?"></a>What is the differences between Image Processing and Computer Vision?</h2><p>The output for Image Processing still is images. For Computer Vision, its output can be images or knowledge.</p>]]></content>
      
      
      <categories>
          
          <category> ImageProcess </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ImageProcess, MachineVision, LectureReview </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Study &#39;Med3D: Transfer Learning for 3D Medical Image Analysis&#39;</title>
      <link href="/2020/08/18/Study-Med3D-Transfer-Learning-for-3D-Medical-Image_Analysis/"/>
      <url>/2020/08/18/Study-Med3D-Transfer-Learning-for-3D-Medical-Image_Analysis/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>By working through the paper ‘Med3D: Transfer Learning for 3D Medical Image Analysis’, I am going to work on 3D image segmentation.</p><p>This paper is trying to solve the following challenge: “It is extremely challenging to build a sufficiently large dataset due to diffculty of data acquisition and annotation in 3D medical imaging.” They build a image data set called 3DSeg-8 by aggregate other dataset. Then they designed Med3D, a heterogeneous 3D network, to pre-train models. As the paper show, Med3D can speed up training covergence and improve accuracy.</p><p>As they said, “The motivation of our work is to train a high performance DCNN model with a relatively large 3D medical dataset, that can be used as the backbone pre-trained model to boost other tasks with insufficient training data.” There are 3 steps they did in their work.</p><h4 id="Step1"><a href="#Step1" class="headerlink" title="Step1:"></a>Step1:</h4><p>Collecting several 3D segmentation datasets and put them together. The new dataset is 3DSeg-8. Then they normalized 3DSeg-8 for both spatial and intensity distribution.</p><h4 id="Step2"><a href="#Step2" class="headerlink" title="Step2:"></a>Step2:</h4><p>Training a DCNN model. They call this model Med3D. This network has same encoder and different decoder branches for each specific dataset.</p><h4 id="Step3"><a href="#Step3" class="headerlink" title="Step3:"></a>Step3:</h4><h2 id="Extracting-feature-from-pre-trained-Med3D-model-Then-trying-with-other-medical-tasks-to-boost-the-network-performance"><a href="#Extracting-feature-from-pre-trained-Med3D-model-Then-trying-with-other-medical-tasks-to-boost-the-network-performance" class="headerlink" title="Extracting feature from pre-trained Med3D model. Then trying with other medical tasks to boost the network performance."></a>Extracting feature from pre-trained Med3D model. Then trying with other medical tasks to boost the network performance.</h2><h2 id="Details-Describe"><a href="#Details-Describe" class="headerlink" title="Details Describe"></a>Details Describe</h2><hr><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a href="https://arxiv.org/abs/1904.00625" target="_blank" rel="noopener">https://arxiv.org/abs/1904.00625</a><br><a href="https://github.com/Tencent/MedicalNet" target="_blank" rel="noopener">https://github.com/Tencent/MedicalNet</a></p>]]></content>
      
      
      <categories>
          
          <category> ImageProcess </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ImageProcess, Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Bank Management System</title>
      <link href="/2020/08/02/Bank-Management-System/"/>
      <url>/2020/08/02/Bank-Management-System/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>2020 summer, I spent some time to work on Java and Python programming. This one is a very simple project I played with for improving Java coding skill. The following Java topic is covered in this project: Class, Parent class, Child class, Abstract class &amp; method, Inheritance, Interface, Static, and other basic Java topics</p><hr><h2 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h2><p>Build a bank management system which can “creat an account”,”log in”, “deposit”,”withdraw”,”check balance”,”transfer”,”loan”,”pay back”,”change password”,”logout”<br>There are four different accounts in this system: 1. Saving account; 2. Credit account; 3. Saving account with loan; 4. Credit account with loan</p><hr><h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><p><img src="/images/bankManagementSystem0.jpg" alt=""><br><img src="/images/bankManagementSystem1.jpg" alt=""><br><img src="/images/bankManagementSystem2.jpg" alt=""></p><hr><h2 id="Coding-details"><a href="#Coding-details" class="headerlink" title="Coding details"></a>Coding details</h2><hr><h2 id="Codes"><a href="#Codes" class="headerlink" title="Codes"></a>Codes</h2><p><a href="https://github.com/TianLuke33/BankManageSystem" target="_blank" rel="noopener">https://github.com/TianLuke33/BankManageSystem</a></p>]]></content>
      
      
      <categories>
          
          <category> Coding </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Themes</title>
      <link href="/2020/07/02/Themes/"/>
      <url>/2020/07/02/Themes/</url>
      
        <content type="html"><![CDATA[<h2 id="3-hexo"><a href="#3-hexo" class="headerlink" title="3-hexo"></a>3-hexo</h2><p>The first theme I am working on is this one. I chose it becasue it has category and comment. Here is some challenges I had while playing with it.</p><ol><li><p>The post category didn’t show out in blog<br>Solution: “_config.yml” in 3-hexo has category and tap setting. However, they won’t work until download hexo category plugin and tag plugin</p></li><li><p>FATAL bad indentation of a mapping entry at line 107, column 2:<br><img src="/images/spaceBug.jpg" alt=""><br>Solution: this error caused by excess blanks in “_config.yml”. Doulbe check the the spacing added.</p></li></ol><hr><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a href="https://github.com/yelog/hexo-theme-3-hexo" target="_blank" rel="noopener">https://github.com/yelog/hexo-theme-3-hexo</a><br><a href="https://github.com/hexojs/hexo-generator-category" target="_blank" rel="noopener">https://github.com/hexojs/hexo-generator-category</a><br><a href="https://github.com/hexojs/hexo-generator-tag" target="_blank" rel="noopener">https://github.com/hexojs/hexo-generator-tag</a></p><div id="gitalk-container"></div>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Study &#39;An Unsupervised Learning Model for Deformable Medical Image Registration&#39;</title>
      <link href="/2020/07/01/Study-An-Unsupervised-Learning-Model-for-Deformable-Medical-Image-Registration/"/>
      <url>/2020/07/01/Study-An-Unsupervised-Learning-Model-for-Deformable-Medical-Image-Registration/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>It is a project I got based on research paper ‘An Unsupervised Learning Model for Deformable Medical Image Registration’. In this paper, authors introduced an algorithm for unsupervised n-D image registration. Image registration is an important task in image processing used in many ways. For this paper, image registration is working with multiple images to find out the changing between them. Their VoxelMorph CNN Architecture is based on UNet. To check the architecture, the following code can be used.<br>“import tensorflow as tf<br> tf.keras.utils.plot_model(vxm_model, show_shapes=True)”<br>They actually have two architectures. One is faster, the other one has higher accuracy. To understand better, the two input images are moving image(M) and fixed image(F). During the training, parameters for function g registers M through F. With spatial transform function, the output is moved image. Then loss function is used to check between moved image and fixed image.</p><hr><h2 id="Test-with-Heart-Image"><a href="#Test-with-Heart-Image" class="headerlink" title="Test with Heart Image"></a>Test with Heart Image</h2><p>The training and testing code should be “train_miccai2018.py” and “test_miccai2018.py”. I don’t have a gpu on my laptop. Therefore, I just run the code on kaggle. After I run the code, I received the following image output. The first line is three slices from Fixed 3D Image(F). It is the second untreated brain image. Second line is Moved Image. It is the output after registration. They are nice and clean<br><img src="/images/brainImage.jpg" alt=""><br>To go deeper, I tried to test with some heart image. I only changed the input images and the parameters about images’ size. I used the same 3D model from the paper which was trainned from brain images. The size of input need to be multipled by 32. The brain image is [160,192,224]. The heart image I used is [64,32,32]. The output is showing in the following image. Yes, it is just part of the heart image. Also, the result doesn’t go very well. The registrated image disappeared. The grey levels are different.<br><img src="/images/errorHeartImage.jpg" alt=""><br>Next step, I am going to use 3D gaussian filter to denoise the heart images and check the plot function.<br>The plot function is “neuron.plot.slices(mid_slices_fixed + mid_slices_pred, cmaps=[‘gray’], do_colorbars=True, grid=[2,3]);” From how they import neuron, I find out the plot.py code under “voxelmorph-master/ext/neuron/neuron”. For gaussian filter, “scipy.ndimage” package is helpful.<br>To check the max element in an array: np.amax()</p><p>I resized the brain image to [64,32,32] to check whether the small size image cost the problem. Then I changed the orientation for the dimensionalities of images. At the end, I sloved the problem by change the grey level of heart images. The grey level for the brain image is under 1. Need to be careful for the input samples run in the test are the updated images.</p><hr><h2 id="Histogram-for-3D-Images"><a href="#Histogram-for-3D-Images" class="headerlink" title="Histogram for 3D Images"></a>Histogram for 3D Images</h2><p>To make sure the heart images are scaled correctly, I need to find out 95% of largest value from their histograms. Then normalize to get new images and register the new images.<br>The heart images are really dark their histograms is similar with the following graph. PS. There are three or four voxels from the original images are above 800. I am not sure why, but I used gaussian filter. Then the maxmum for these arraies are below 255. img.astype(‘int’) can be used to change data type in array.<br><img src="/images/heartHistogram.jpg" alt=""></p><hr><h2 id="One-mistake-and-updating"><a href="#One-mistake-and-updating" class="headerlink" title="One mistake and updating"></a>One mistake and updating</h2><p>After contact with my professor, I noticed the output results are incorrect. The registrated images actually disappeared. The reason to cost this problem is I did not input the normalized images. After normalized, the gray level for images are all between 0 and 1. The following code is how I normalized all of these images.</p><p>max1 = np.amax(heart_image_1)<br>max2 = np.amax(heart_image_2)<br>min1 = np.amin(heart_image_1)<br>min2 = np.amin(heart_image_2)<br>print(max1,max2,min1,min2)<br>heart_image_1 = (heart_image_1-min1)/(max1-min1)<br>heart_image_2 = (heart_image_2-min2)/(max2-min2)</p><p>Instead of remove 95% of brightest pixels. I actually should only remove 1% of brightest pixels. The code I used is showing here:</p><p>greyLevel1 = heart_image_1.ravel()<br>greyLevel2 = heart_image_2.ravel()<br>newGrey1 = np.sort(-greyLevel1)<br>newGrey2 = np.sort(-greyLevel2)<br>[a,b,c] = heart_image_1.shape<br>for x in range(a):<br>    for y in range(b):<br>        for z in range(c):<br>            if heart_image_1[x,y,z] &gt; (-newGrey1[656]):<br>                heart_image_1[x,y,z] = (-newGrey1[656])<br>for d in range(a):<br>    for e in range(b):<br>        for f in range(c):<br>            if heart_image_2[d,e,f] &gt; (-newGrey2[656]):<br>               heart_image_2[d,e,f] = (-newGrey2[656])</p><hr><h2 id="Removing-1-brightest-pixels-VS-Gaussian-filter"><a href="#Removing-1-brightest-pixels-VS-Gaussian-filter" class="headerlink" title="Removing 1% brightest pixels VS. Gaussian filter"></a>Removing 1% brightest pixels VS. Gaussian filter</h2><p>Next step, I compared two different ways to denoise heart images. One is removing 1% brightest pixels before normalized and registrated. The other one is using Gaussian filter to denoise before normalized and registrated. Package scipy.ndimage has 3D gaussian filter. With the package code is very simple:</p><p>from scipy.ndimage import gaussian_filter<br>heart_image_1 = gaussian_filter(heart_image_1,sigma=1)<br>heart_image_2 = gaussian_filter(heart_image_2,sigma=1)</p><p>The output shows gaussian filter has lower registrated error. But the images are fuzzier because it over denoised. Even though removing 1% brightest pixels has higher error data. It still is a better way for image processing.</p><hr><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>It is a summer project I worked in 2020. For me, it is a chance understand image registration and try normalization, gaussian filter,etc. This project, itself, is not challengeable. The model was finished by others. It still is an enter level work for me to go through and study these topics.</p><hr><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Balakrishnan_An_Unsupervised_Learning_CVPR_2018_paper.pdf" target="_blank" rel="noopener">https://openaccess.thecvf.com/content_cvpr_2018/papers/Balakrishnan_An_Unsupervised_Learning_CVPR_2018_paper.pdf</a><br><a href="https://github.com/voxelmorph/voxelmorph" target="_blank" rel="noopener">https://github.com/voxelmorph/voxelmorph</a></p>]]></content>
      
      
      <categories>
          
          <category> ImageProcess </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ImageProcess, Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>About myself</title>
      <link href="/2020/07/01/About-myself/"/>
      <url>/2020/07/01/About-myself/</url>
      
        <content type="html"><![CDATA[<h2 id="July-2020"><a href="#July-2020" class="headerlink" title="July 2020"></a>July 2020</h2><p>I am a master degree student in EE now. After college, I worked 2 years as hardware engineer and innovation engineer. I came back to school again because I am curious about machine version and I want to improve my software skills. I did not choose to study CS because I cannot picture myself as a programmer. In my mind, I want to be the engineer to finish products instead of just get my pieces of work done. At this moment, I am thinking to work as innovation engineer, design engineer, or EE. That means I need to learn engineering knowledge in different areas. Therefore, I design to build this blog to record the tasks I got and the lessons I learned. It is how this began.</p><hr>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
